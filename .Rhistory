library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
library(academictwitteR) # for fetching Twitter data
getwd()
newspapers = c("TheSun", "DailyMailUK", "MetroUK", "DailyMirror",
"EveningStandard", "thetimes", "Telegraph", "guardian")
tweets <-
get_all_tweets(
users = newspapers,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-05-01T00:00:00Z",
data_path = "data/sentanalysis/",
n = Inf,
)
head(tweets)
getwd()
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username,
newspaper = user_name,
tweet = text)
tweets %>%
arrange(created_at) %>%
tail(5) %>%
kbl() %>%
kable_styling(c("striped", "hover", "condensed", "responsive"))
tidy_tweets <- tweets %>%
mutate(desc = tolower(tweet)) %>%
unnest_tokens(word, desc) %>%
filter(str_detect(word, "[a-z]"))
tidy_tweets <- tidy_tweets %>%
filter(!word %in% stop_words$word)
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
nrc_fear <- get_sentiments("nrc") %>%
filter(sentiment == "fear") #looking in nrc for fear words
tidy_tweets %>%
inner_join(nrc_fear) %>%
count(word, sort = TRUE) #for those where it is true
#get tweet sentiment by date
tweets_nrc_sentiment <- tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>% #add in the sentiment dictionary
count(date, index = order %/% 1000, sentiment) %>% #specify we count over dates, index = order (by row number), over every 1000 rows (every 1000 words)
spread(sentiment, n, fill = 0) %>% #spread to convert the sentiment scores for our sentiment types into columns (not rows)
mutate(sentiment = positive - negative)#calc sentiment score sub neg sentiment from pos sentiment
#gen data variable, order and format date
tidy_tweets$date <- as.Date(tidy_tweets$created_at)
tidy_tweets <- tidy_tweets %>%
arrange(date)
tidy_tweets$order <- 1:nrow(tidy_tweets)
#get tweet sentiment by date
tweets_nrc_sentiment <- tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>% #add in the sentiment dictionary
count(date, index = order %/% 1000, sentiment) %>% #specify we count over dates, index = order (by row number), over every 1000 rows (every 1000 words)
spread(sentiment, n, fill = 0) %>% #spread to convert the sentiment scores for our sentiment types into columns (not rows)
mutate(sentiment = positive - negative)#calc sentiment score sub neg sentiment from pos sentiment
tweets_nrc_sentiment %>%
ggplot(aes(date, sentiment)) + #plot it in ggplot
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25)
tidy_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("bing sentiment")
tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>%
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("nrc sentiment")
tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>%
group_by(date, index = order %/% 1000) %>%
summarise(sentiment = sum(value)) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("afinn sentiment")
#create a list of words associated to mortality
word <- c('death', 'illness', 'hospital', 'life', 'health',
'fatality', 'morbidity', 'deadly', 'dead', 'victim')
value <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1) #assign a value to the words 1 being mortality related
mordict <- data.frame(word, value) #make dictionary/data frame with word n value
mordict
tidy_tweets %>% #bind mordict to data and look at instances over time
inner_join(mordict) %>%
group_by(date, index = order %/% 1000) %>%
summarise(morwords = sum(value)) %>%
ggplot(aes(date, morwords)) +
geom_bar(stat= "identity") +
ylab("mortality words")
mordict <- c('death', 'illness', 'hospital', 'life', 'health',
'fatality', 'morbidity', 'deadly', 'dead', 'victim')
#get total tweets per day (no missing dates so no date completion required)
totals <- tidy_tweets %>%
mutate(obs=1) %>%
group_by(date) %>%
summarise(sum_words = sum(obs))
#plot
tidy_tweets %>%
mutate(obs=1) %>%
filter(grepl(paste0(mordict, collapse = "|"),word, ignore.case = T)) %>%
group_by(date) %>%
summarise(sum_mwords = sum(obs)) %>%
full_join(totals, word, by="date") %>% #include dates whre there are no mort word, you dont want to exclude
mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
pctmwords = sum_mwords/sum_words) %>%
ggplot(aes(date, pctmwords)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
xlab("Date") + ylab("% mortality words")
tweets$date <- as.Date(tweets$created_at)#make sure date column stored correctly
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date") #make a corpus object for use in quanteda pkge, docvars includes info on when tweet was published
tweets$date <- as.Date(tweets$created_at)#make sure date column stored correctly
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date") #make a corpus object for use in quanteda pkge, docvars includes info on when tweet was published
toks_news <- tokens(tweet_corpus, remove_punct = TRUE) #tokenize the text and remove punctuation
tweets$date <- as.Date(tweets$created_at)#make sure date column stored correctly
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "Date") #make a corpus object for use in quanteda pkge, docvars includes info on when tweet was published
#WARNING 'docvars' argument is not used
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
library(academictwitteR) # for fetching Twitter data
getwd()
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username, #changes user_username to username
newspaper = user_name, #user_name to newspaper
tweet = text) #text to tweet
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
library(academictwitteR) # for fetching Twitter data
getwd()
# This is a code chunk to show the code that collected the data using the twitter API, back in 2020.
# You don't need to run this, and this chunk of code will be ignored when you knit to html, thanks to the 'eval=FALSE' command in the chunk option.
newspapers = c("TheSun", "DailyMailUK", "MetroUK", "DailyMirror",
"EveningStandard", "thetimes", "Telegraph", "guardian")
tweets <-
get_all_tweets(
users = newspapers,
start_tweets = "2020-01-01T00:00:00Z",
end_tweets = "2020-05-01T00:00:00Z",
data_path = "data/sentanalysis/",
n = Inf,
)
library(kableExtra)
library(tidyverse) # loads dplyr, ggplot2, and others
library(readr) # more informative and easy way to import data
library(stringr) # to handle text elements
library(tidytext) # includes set of functions useful for manipulating text
library(quanteda) # includes functions to implement Lexicoder
library(textdata)
library(academictwitteR) # for fetching Twitter data
getwd()
tweets  <- readRDS(gzcon(url("https://github.com/cjbarrie/CTA-ED/blob/main/data/sentanalysis/newstweets.rds?raw=true")))
head(tweets)
colnames(tweets)
tweets <- tweets %>%
select(user_username, text, created_at, user_name,
retweet_count, like_count, quote_count) %>%
rename(username = user_username, #changes user_username to username
newspaper = user_name, #user_name to newspaper
tweet = text) #text to tweet
tweets %>%
arrange(created_at) %>% #takes the tweets data frame and arranges its rows in ascending order based on the created_at column, sorts the tweets from the earliest to the latest
tail(5) %>% #selects the last 5 rows of the sorted data frame (the five most recent tweets)
kbl() %>% #turns r dataframe into formatted table
kable_styling(c("striped", "hover", "condensed", "responsive")) #styling options withing the dataframe
tidy_tweets <- tweets %>%
mutate(desc = tolower(tweet)) %>%
unnest_tokens(word, desc) %>%
filter(str_detect(word, "[a-z]"))
tidy_tweets <- tidy_tweets %>%
filter(!word %in% stop_words$word) #filter out words in stop_words
get_sentiments("afinn") #uses -5 to 5, no zero
get_sentiments("bing")#negative vs positive, binary classification
get_sentiments("nrc")#multiple word associations for the same word, like abandon  "fear, negative , sadness", huge lib
nrc_fear <- get_sentiments("nrc") %>%
filter(sentiment == "fear") #looking in nrc for fear words
tidy_tweets %>%
inner_join(nrc_fear) %>% #filter the tweet data
count(word, sort = TRUE) #for those where it is true, return the word and the number of times it apprear? what is N
#gen data variable, order and format date
tidy_tweets$date <- as.Date(tidy_tweets$created_at)
tidy_tweets <- tidy_tweets %>%
arrange(date)
tidy_tweets$order <- 1:nrow(tidy_tweets)
#get tweet sentiment by date
tweets_nrc_sentiment <- tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>% #add in the sentiment dictionary
count(date, index = order %/% 1000, sentiment) %>% #specify we count over dates, index = order (by row number), over every 1000 rows (every 1000 words)
spread(sentiment, n, fill = 0) %>% #spread to convert the sentiment scores for our sentiment types into columns (not rows)
mutate(sentiment = positive - negative)#calc sentiment score sub neg sentiment from pos sentiment
tweets_nrc_sentiment %>%
ggplot(aes(date, sentiment)) + #plot it in ggplot date vs sentiment
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25)
tidy_tweets %>%
inner_join(get_sentiments("bing")) %>% #BING dictionary
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("bing sentiment")
tidy_tweets %>%
inner_join(get_sentiments("nrc")) %>% #NRC dictionary
count(date, index = order %/% 1000, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) +
geom_smooth(method= loess, alpha=0.25) +
ylab("nrc sentiment")
tidy_tweets %>%
inner_join(get_sentiments("afinn")) %>% #AFINN
group_by(date, index = order %/% 1000) %>%
summarise(sentiment = sum(value)) %>%
ggplot(aes(date, sentiment)) +
geom_point(alpha=0.5) + #scatter plot
geom_smooth(method= loess, alpha=0.25) +
ylab("afinn sentiment")
#create a list of words associated to mortality
word <- c('death', 'illness', 'hospital', 'life', 'health',
'fatality', 'morbidity', 'deadly', 'dead', 'victim')
value <- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1) #assign a value to the words 1 being mortality related
mordict <- data.frame(word, value) #make dictionary/data frame with word n value
mordict
tidy_tweets %>% #bind mordict to data and look at instances over time
inner_join(mordict) %>%
group_by(date, index = order %/% 1000) %>%
summarise(morwords = sum(value)) %>%
ggplot(aes(date, morwords)) +
geom_bar(stat= "identity") + #barchart / freq chart
ylab("mortality words")
mordict <- c('death', 'illness', 'hospital', 'life', 'health',
'fatality', 'morbidity', 'deadly', 'dead', 'victim')
#get total tweets per day (no missing dates so no date completion required)
totals <- tidy_tweets %>%
mutate(obs=1) %>%
group_by(date) %>%
summarise(sum_words = sum(obs))
#plot
tidy_tweets %>%
mutate(obs=1) %>%
filter(grepl(paste0(mordict, collapse = "|"),word, ignore.case = T)) %>%
group_by(date) %>%
summarise(sum_mwords = sum(obs)) %>%
full_join(totals, word, by="date") %>% #include dates whre there are no mort word, you dont want to exclude
mutate(sum_mwords= ifelse(is.na(sum_mwords), 0, sum_mwords),
pctmwords = sum_mwords/sum_words) %>%
ggplot(aes(date, pctmwords)) +
geom_point(alpha=0.5) + #scatter plot
geom_smooth(method= loess, alpha=0.25) + #line of best fit
xlab("Date") + ylab("% mortality words") #x n y axes
tweets$date <- as.Date(tweets$created_at)#make sure date column stored correctly
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date") #make a corpus object for use in quanteda pkge, docvars includes info on when tweet was published
#WARNING 'docvars' argument is not used
toks_news <- tokens(tweet_corpus, remove_punct = TRUE) #tokenize the text and remove punctuation
tweets$date <- as.date(tweets$created_at)#make sure date column stored correctly
tweets$date <- as.Date(tweets$created_at)#make sure date column stored correctly
tweet_corpus <- corpus(tweets, text_field = "tweet", docvars = "date") #make a corpus object for use in quanteda pkge, docvars includes info on when tweet was published
#WARNING 'docvars' argument is not used
